{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8985e12a-9282-4a40-b62e-7a598ae375bb",
   "metadata": {},
   "source": [
    "# Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92dc59d3-5542-4335-89fd-03c3f45cc988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93058547-7658-4243-b072-8482cd6514bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'our nation faces a profound climate crisis that is impacting american'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentance = 'our nation faces a profound climate crisis that is impacting american'\n",
    "sentance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2657ff74-d001-4286-b326-ca4ab3d786e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word(word, sentance):\n",
    "    return word in sentance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "864bdec1-1be1-4a63-91de-0b6aa8b2faa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_word('profound', sentance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32eb5e56-a778-4a2b-ad23-d6f7e95ba06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(word, text):\n",
    "    return text.index(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90c81220-4c66-4e9c-9687-6721b4d74f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_index('crisis', sentance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09e07232-c754-43f2-b0b1-fa17f9fb5a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_index('american', sentance.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32c4a054-7170-4cc5-ab2a-fd9e4c1ffa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word(text, rank):\n",
    "    return text.split()[rank]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "800b97ba-e1c0-41ed-be55-e7710fe18021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'profound'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word(sentance, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c265a2e-aeb4-46c1-9475-7d93473f0d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dnuoforp'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word(sentance,4)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b16ee34c-35a0-4c82-93ac-4fecb02d6d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def con_words(text):\n",
    "    words = text.split()\n",
    "    print(words)\n",
    "\n",
    "    first_word  = text.split()[0]\n",
    "    print(first_word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1333953-4a5b-495a-8f55-f93be1572491",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Natural Language processing is the powerful concept for the sentiment analysis, this is utilized by many companies at the current scenario'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0801d21b-501d-4b6f-a311-bd06c7aa60be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word(word, sentence):\n",
    "    return word in sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af4126ac-0587-47b0-ac63-fb5c7ed3710c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#Call the function\n",
    "ret_obj = find_word('Natural',sentence)\n",
    "print(ret_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "512fdd40-9baf-4dfa-8bab-a8a99896e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(word,text):\n",
    "    return text.index(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83af8b1d-affe-4d33-a7d2-c94ae6e0f54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "obj_index = get_index('Natural',sentence)\n",
    "print(obj_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2af7794d-1731-4476-9879-5af0fe75526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_even_position_words(text):\n",
    "    words = text.split()\n",
    "    return [words[i] for i in range(len(words)) if i%2 == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d7ea1f8-7a21-443d-9b0c-400a29ab166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Natural Language processing is the powerful concept for the sentiment analysis, this is utilized by many companies at the current scenario'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5eeece98-9f28-4ad7-94a7-a82c1f4f1721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'processing', 'the', 'concept', 'the', 'analysis,', 'is', 'by', 'companies', 'the', 'scenario']\n"
     ]
    }
   ],
   "source": [
    "#Call the function\n",
    "even_pos_output = get_even_position_words(sentence)\n",
    "print(even_pos_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eaaae00c-7456-4e5d-80aa-62f89dc243fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reverse(text):\n",
    "    return text[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a275e810-4773-430c-86d0-6933391aede8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oiranecs tnerruc eht ta seinapmoc ynam yb dezilitu si siht ,sisylana tnemitnes eht rof tpecnoc lufrewop eht si gnissecorp egaugnaL larutaN\n"
     ]
    }
   ],
   "source": [
    "reverse_out = get_reverse(sentence)\n",
    "print(reverse_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345e6887-5ac5-451c-8361-020da6473d3e",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6222c557-266a-435f-a085-afb3c9b551ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\udaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\udaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\udaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import the relevant packages to split the words \n",
    "from nltk import word_tokenize, download\n",
    "download(['punkt', 'averaged_perceptron_tagger', 'stopwords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57daef95-8532-44a7-905b-ef9b74776d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the below sentence to get the output. \n",
    "Sentence = 'Natural Langauge processing is one of the most important concepts in the industry.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ace6966-7500-4476-9c5c-3a32b9352956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the function which will detect the boundaries of the sentences\n",
    "def get_tokens(text):\n",
    "    words = word_tokenize(Sentence)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64fad202-1402-4215-ac8c-76cf08a7b508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Langauge', 'processing', 'is', 'one', 'of', 'the', 'most', 'important', 'concepts', 'in', 'the', 'industry', '.']\n"
     ]
    }
   ],
   "source": [
    "#Call the function to get the output\n",
    "Sentence = 'Natural Langauge processing is one of the most important concepts in the industry.'\n",
    "output = get_tokens(Sentence)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e125bd-2e04-4777-8905-a87b172dbede",
   "metadata": {},
   "source": [
    "# Sentence Boundary Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e0ec342-874c-4f6b-b68b-4d16f6ea3ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec11ae73-30a5-40e2-b9f4-f3c682e9ff12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(text):\n",
    "    return sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7dce6d2-01e2-4e89-9d71-ad6831e565ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural Language Processing is one of the most powerful concepts.', 'It is the future']\n"
     ]
    }
   ],
   "source": [
    "Sentence = 'Natural Language Processing is one of the most powerful concepts. It is the future'\n",
    "output = get_sentences(Sentence)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f28f97-21d8-4c87-9c74-133fbc4d1ade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4869d08-42ff-4c3b-b7ad-2d403fb76631",
   "metadata": {},
   "source": [
    "# PoS tagging\n",
    "\n",
    "Part-of-Speech (PoS) Tagging is the process of assigning parts of speech to individual words in a sentence, based on their context and definition. It identifies whether each word is a noun, verb, adjective, adverb, etc., which helps in understanding the structure and meaning of the sentence in Natural Language Processing (NLP) tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c7ad64-bfaa-41aa-9e15-dd6485f31833",
   "metadata": {},
   "source": [
    "# Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7017ba4-b971-47be-be12-78e6cce50bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\udaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import download\n",
    "download('stopwords')\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2931cd7b-005b-4ba1-a131-92c85b2151d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('English')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77af928e-bf9d-4d97-a7b6-f46b04205b34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57d3b5fc-2c08-46b8-8f88-964a0701610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I'm learning python. It is one of the most popular programming languages.\"\n",
    "sentence_words = word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "17fdfe28-3587-434c-86c9-7ac87998816a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'m\", 'learning', 'python', '.', 'It', 'is', 'one', 'of', 'the', 'most', 'popular', 'programming', 'languages', '.']\n"
     ]
    }
   ],
   "source": [
    "print(sentence_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc35cb94-368a-4185-a53c-694438b9fdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(sentence, stop_words):\n",
    "    return ' '.join([word for word in sentence if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6e3346b9-af92-4768-b81f-1173dd336f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 'm learning python . It one popular programming languages .\n"
     ]
    }
   ],
   "source": [
    "print(remove_stop_words(sentence_words, stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29d0e0dd-3ce8-4880-a8b7-d29bb0d7b390",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m stop_words\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mone\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(remove_stop_words(sentence_words, stop_words))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stop_words' is not defined"
     ]
    }
   ],
   "source": [
    "stop_words.extend([\"I\", \"It\", \"one\", \"'m\", '.'])\n",
    "print(remove_stop_words(sentence_words, stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42fc3c27-71e9-4c20-a18e-4c6ae7b25c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first met quiet. remained quiet entire two-hour-long journey Stony Brook New York.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\udaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "sw = stopwords.words('english')\n",
    "text = \"When I first met her she was very quiet. She remained quiet during the entire two-hour-long journey from Stony Brook to New York.\"\n",
    "words = [word for word in text.split() if word.lower() not in sw]\n",
    "new_text = \" \".join(words)\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a128896-a06b-4098-b19d-2f48751ba3bc",
   "metadata": {},
   "source": [
    "# Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bbf4678c-f1a3-4e08-8e2e-ce331aadec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I visited the India from UK on 22-10-18\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e37f462a-ade9-4e97-8a04-d665fc098ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    return text.replace('UK' , 'United Kingdom').replace('-18', '-2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e89fe74-6c5f-44a9-ad2b-a3196e393568",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_text = normalize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5a2af80e-f224-4c02-a2f3-d49f7d1b76a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I visited the India from United Kingdom on 22-10-2018\n"
     ]
    }
   ],
   "source": [
    "print(normalize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fc22b4-ef93-490c-8069-d9b262b43d38",
   "metadata": {},
   "source": [
    "# Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0cb2655f-6ee4-4b17-a1d3-23c9a07eae99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autocorrect in c:\\users\\udaya\\anaconda3\\lib\\site-packages (2.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1c542be5-ed3f-4d1a-b1ae-07e186626732",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from autocorrect import Speller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "15049130-bb30-4709-9a5b-e3ffa9890928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell = Speller(lang='en')\n",
    "spell('Natureal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6585405e-db64-4343-97b7-1001b0ac69b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural',\n",
       " 'language',\n",
       " 'processinf',\n",
       " 'delas',\n",
       " 'eith',\n",
       " 'art',\n",
       " 'of',\n",
       " 'extracving',\n",
       " 'insights',\n",
       " 'from',\n",
       " 'languages']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'Natural language processinf  delas eith art of extracving  insights from languages'\n",
    "sentence = word_tokenize(sentence)\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "70429493-ad78-4a49-8795-a56a8f2c8b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_spelling(tokens):\n",
    "    sentence_corrected = ' '.join([spell(word) for word in tokens])\n",
    "    return sentence_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "66c1d92c-450c-4ecd-8c03-ac6f5f1ddea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing deals with art of extracting insights from languages\n"
     ]
    }
   ],
   "source": [
    "print(correct_spelling(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc57b82-87f9-4afe-90ae-ea1a5a4d8be5",
   "metadata": {},
   "source": [
    "# Stemming:\n",
    "- In Natural Language Processing (NLP), stemming is the process of reducing a word to its root or base form, known as the \"stem.\" The goal of stemming is to simplify the analysis by treating different forms of a word (like plurals or different tenses) as the same word. This is particularly useful for tasks like search engines, where the various forms of a word should match the same query.\n",
    " \n",
    "Common Stemming Algorithms:\n",
    "- Porter Stemmer: One of the most commonly used algorithms, developed in 1980. It uses a series of rules to remove common English suffixes.\n",
    "- Snowball Stemmer: Also known as the \"Porter2\" stemmer, itâ€™s an improved version of the Porter Stemmer and supports multiple languages.\n",
    "- Lancaster Stemmer: A more aggressive and faster stemmer than the Porter Stemmer but often reduces words to overly short stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fed04c96-be32-46c7-9ff4-4624becdb52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "93d3379e-1078-46db-9c87-f2aa97842055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stemms(word, stemmer):\n",
    "    return stemmer.stem(word)\n",
    "porterstem = stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3c1de067-2393-410c-9503-6987e2514349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'product'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stemms('producted', porterstem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4fefc3ab-5c4e-4fdf-ba10-0c6bc219014e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'come'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stemms('coming', porterstem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8584dedf-9e52-4543-8097-0570dbe70d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'program'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stemms('programming', porterstem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "54c7cbfc-3239-4fbf-8be4-d5d8a76d9b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2419ef34-a933-42b7-a9d3-a9716cd60c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "para = '''process of reducing a word to its root or base form, known as the stem. \n",
    "The goal of stemming is to simplify the analysis by treating different forms of a word (like plurals or different tenses) as the same word.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "285afe6e-b0f5-458e-bcdd-158ca4888dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "para_tokens = word_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "04abcb62-fa3e-4551-827e-bbdc7bddbe39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['process', 'of', 'reducing', 'a', 'word', 'to', 'its', 'root', 'or', 'base', 'form', ',', 'known', 'as', 'the', 'stem', '.', 'The', 'goal', 'of', 'stemming', 'is', 'to', 'simplify', 'the', 'analysis', 'by', 'treating', 'different', 'forms', 'of', 'a', 'word', '(', 'like', 'plurals', 'or', 'different', 'tenses', ')', 'as', 'the', 'same', 'word', '.']\n"
     ]
    }
   ],
   "source": [
    "print(para_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f160aa66-6962-431b-bd63-762cb6733b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process reducing word root base form , known stem The goal stemming simplify analysis treating different forms word ( like plurals different tenses ) word\n"
     ]
    }
   ],
   "source": [
    "#remove stop words\n",
    "para_sw = remove_stop_words(para_tokens, stop_words)\n",
    "print(para_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "330e22fe-892f-4c7b-ba35-5a568ab08109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'process reducing word root base form , known stem the goal stemming simplify analysis treating different forms word ( like plurals different tenses ) word'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stemms(para_sw, porterstem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8e42a9-4811-4992-9cba-3b7e79f44b62",
   "metadata": {},
   "source": [
    "# Lemmitization\n",
    "Lemmatization in Natural Language Processing (NLP) is the process of reducing words to their base or root form (lemma), considering the context and grammatical meaning of the word. Unlike stemming, which simply cuts off prefixes or suffixes based on predefined rules, lemmatization considers the part of speech and the meaning of the word to return a valid base form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1cd71b-af44-4584-a0be-e2c8611602a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c2904870-5af9-4135-9570-9f1d9effdac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\udaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import download\n",
    "download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3ed28b82-3e94-46fb-bd78-318b3d511a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmitizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d55ba778-eb4c-4230-b5b4-6a1101ee098b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma(word):\n",
    "    return lemmitizer.lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4c0086a2-787e-4f1e-8369-9988077b9bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'product'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_lemma('products')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "604197bd-7845-45e3-acdc-495e0ade1821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'production'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_lemma('production')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "992f13ce-d192-46eb-bfc9-8dfa3ebdce21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cooling'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_lemma('cooling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f5930883-2370-42af-add9-ac39a2ddb790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stemms('loved', stemmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf908d21-1450-423d-8d4f-1582cdeb6b89",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER)\n",
    "Named Entity Recognition (NER) is a Natural Language Processing (NLP) technique that identifies and classifies named entities in text into predefined categories such as persons, organizations, locations, dates, and more. NER is often used in information extraction to structure unstructured data, enabling machines to understand which entities are being referred to in a text.\n",
    "\n",
    "Common NER Categories:\n",
    "\n",
    "- Person: Identifies names of individuals (e.g., \"Elon Musk\").\n",
    "- Organization: Recognizes company or institution names (e.g., \"Google,\" \"UN\").\n",
    "- Location: Detects geographical locations (e.g., \"New York City,\" \"Brazil\").\n",
    "- Date and Time: Finds dates and times (e.g., \"January 2024,\" \"5:00 PM\").\n",
    "- Monetary Values: Recognizes prices, salaries, etc. (e.g., \"$5000\").\n",
    "- Percentages: Identifies percentage figures (e.g., \"80%\").\n",
    "- Miscellaneous Entities: Categories such as product names, titles of works, or nationalities.\n",
    "\n",
    "Example of NER:\n",
    "\n",
    "Given the sentence: \"Apple is looking to acquire a startup in San Francisco by next January.\"\n",
    "\n",
    "The NER system would recognize:\n",
    "\n",
    "Apple (Organization)\n",
    "San Francisco (Location)\n",
    "next January (Date)\n",
    "\n",
    "Applications of NER:\n",
    "\n",
    "- Information Retrieval: Extracting structured data from news articles, legal documents, or medical records.\n",
    "- Question Answering Systems: Identifying and extracting key entities from user queries to provide better answers.\n",
    "- Customer Service Automation: Automatically detecting names, order numbers, and locations in customer messages.\n",
    "- Text Summarization: Extracting important entities to form a concise summary of the document.\n",
    "- Sentiment Analysis: Identifying the subjects (e.g., brands, people) to better analyze sentiment toward those entities.\n",
    "\n",
    "NER Methods:\n",
    "\n",
    "- Rule-based NER: Uses hand-crafted rules, regular expressions, and dictionaries to recognize entities. This approach is effective for limited and - - - specific domains but doesn't generalize well.\n",
    "- Statistical NER: Relies on probabilistic models like Hidden Markov Models (HMMs) and Conditional Random Fields (CRFs), trained on labeled data to - - predict named entities.\n",
    "\n",
    "Deep Learning-based NER: \n",
    "\n",
    "- Modern approaches use deep learning models (e.g., Recurrent Neural Networks, BERT-based models) to achieve state-of-the-art performance by learning complex patterns in the text.\n",
    "\n",
    "Tools and Libraries for NER:\n",
    "\n",
    "- spaCy: A popular NLP library that includes a pre-trained NER model.\n",
    "- NLTK: Provides NER capabilities with the help of classifiers and taggers.\n",
    "- Stanford NER: A Java-based tool that supports fine-tuned NER tasks.\n",
    "- Hugging Face Transformers: Pre-trained models like BERT can be fine-tuned for NER tasks using deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f839276f-d562-4b07-81a4-7f72c5d8bf29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "816fe3f4-186b-420f-baf6-c9a22deaa15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\udaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\udaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import download\n",
    "from nltk import pos_tag\n",
    "from nltk import ne_chunk\n",
    "from nltk import word_tokenize\n",
    "download('maxent_ne_chunker')\n",
    "download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fef48720-87fc-475f-8019-24effc339e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentance = 'we are reading a book published by novels which is based out of Andhra pradesh, India'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f84ce2c1-b19a-4ad1-92d4-a9966e3e5f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ner(text):\n",
    "    i = ne_chunk(pos_tag(word_tokenize(text)), binary=True)\n",
    "    return[a for a in i if len(a)==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0a6c6558-a913-4fa6-b54d-8b7266f96e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('NE', [('Andhra', 'NNP')]), Tree('NE', [('India', 'NNP')])]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ner(sentance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea4f24d-1380-4d30-b6a1-bc0c49b14b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f6c75b9f-648f-4f4e-be76-e80a667c91e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'display' from 'spacy' (C:\\Users\\udaya\\anaconda3\\Lib\\site-packages\\spacy\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'display' from 'spacy' (C:\\Users\\udaya\\anaconda3\\Lib\\site-packages\\spacy\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from spacy import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b731910-b86e-4f37-9fee-182c4554a5c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2147120a-18c8-402f-ab16-0261a104f557",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spacy' is not defined"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb46e039-807a-4c18-bb8d-2299488838a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ce8d05-bc72-483d-8467-1c61b009f977",
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
