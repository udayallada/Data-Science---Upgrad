{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeeb991d-4c43-41fc-810d-2937bee794c6",
   "metadata": {},
   "source": [
    "# TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "575161d1-40a9-4c62-8ce9-015870fd532b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: TextBlob in c:\\users\\udaya\\anaconda3\\lib\\site-packages (0.18.0.post0)\n",
      "Requirement already satisfied: nltk>=3.8 in c:\\users\\udaya\\anaconda3\\lib\\site-packages (from TextBlob) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\udaya\\anaconda3\\lib\\site-packages (from nltk>=3.8->TextBlob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\udaya\\anaconda3\\lib\\site-packages (from nltk>=3.8->TextBlob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\udaya\\anaconda3\\lib\\site-packages (from nltk>=3.8->TextBlob) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\udaya\\anaconda3\\lib\\site-packages (from nltk>=3.8->TextBlob) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\udaya\\anaconda3\\lib\\site-packages (from click->nltk>=3.8->TextBlob) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b766318d-94b3-4cd0-8299-7e2cdb9bfa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a27858e7-c742-4df3-95af-e0bc0e57f577",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytxt = \"\"\"The TextBlob library is used for processing textual data. \n",
    "TextBlob is a Python (2 and 3) library that provides a simple API for diving into common natural language processing (NLP) tasks.\n",
    "The features of the TextBlob library include parts-of-speech tagging, word inflection, word and phrase frequencies, tokenization, and \n",
    "spelling correction is used to add on top of a method to send data.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73f7c0e6-de69-4954-9b69-0c6b0b112037",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = TextBlob(mytxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf17beba-765a-4a8c-8c82-0d2a7307fdb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"The TextBlob library is used for processing textual data. \n",
       "TextBlob is a Python (2 and 3) library that provides a simple API for diving into common natural language processing (NLP) tasks.\n",
       "The features of the TextBlob library include parts-of-speech tagging, word inflection, word and phrase frequencies, tokenization, and \n",
       "spelling correction is used to add on top of a method to send data.\")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a491cfc-6dfc-452c-ba5e-e6b92e8b46b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textblob.blob.TextBlob"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9f72d0f3-94a7-44ee-acd9-896c19fe8799",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob.blob import BaseBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e3913c06-1ee5-4c36-905c-01c18da368f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseBlob(\"The TextBlob library is used for processing textual data. \n",
       "TextBlob is a Python (2 and 3) library that provides a simple API for diving into common natural language processing (NLP) tasks.\n",
       "The features of the TextBlob library include parts-of-speech tagging, word inflection, word and phrase frequencies, tokenization, and \n",
       "spelling correction is used to add on top of a method to send data.\")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bblob = BaseBlob(mytxt)\n",
    "bblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "83855589-b9e5-49a5-8223-46a6ade0cac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"The TextBlob library is used for processing textual data. \n",
       "TextBlob is a Python (2 and 3) library that provides a simple API for diving into common natural language processing (NLP) tasks.\n",
       "The features of the TextBlob library include parts-of-speech tagging, word inflection, word and phrase frequencies, tokenization, and \n",
       "spelling correction is used to add on top of a method to send data.\")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e8c5eaf7-7451-4083-a704-108dd5014de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['The', 'TextBlob', 'library', 'is', 'used', 'for', 'processing', 'textual', 'data', 'TextBlob', 'is', 'a', 'Python', '2', 'and', '3', 'library', 'that', 'provides', 'a', 'simple', 'API', 'for', 'diving', 'into', 'common', 'natural', 'language', 'processing', 'NLP', 'tasks', 'The', 'features', 'of', 'the', 'TextBlob', 'library', 'include', 'parts-of-speech', 'tagging', 'word', 'inflection', 'word', 'and', 'phrase', 'frequencies', 'tokenization', 'and', 'spelling', 'correction', 'is', 'used', 'to', 'add', 'on', 'top', 'of', 'a', 'method', 'to', 'send', 'data'])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.words #word tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "70b94d4f-99a4-4298-adc9-4b2636acf496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"The TextBlob library is used for processing textual data.\"),\n",
       " Sentence(\"TextBlob is a Python (2 and 3) library that provides a simple API for diving into common natural language processing (NLP) tasks.\"),\n",
       " Sentence(\"The features of the TextBlob library include parts-of-speech tagging, word inflection, word and phrase frequencies, tokenization, and \n",
       " spelling correction is used to add on top of a method to send data.\")]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.sentences #sentence tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6eea66b6-79a9-4eb9-83f9-e04259e0b32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'TextBlob', 'library', 'is', 'used', 'for', 'processing', 'textual', 'data']\n",
      "['TextBlob', 'is', 'a', 'Python', '2', 'and', '3', 'library', 'that', 'provides', 'a', 'simple', 'API', 'for', 'diving', 'into', 'common', 'natural', 'language', 'processing', 'NLP', 'tasks']\n",
      "['The', 'features', 'of', 'the', 'TextBlob', 'library', 'include', 'parts-of-speech', 'tagging', 'word', 'inflection', 'word', 'and', 'phrase', 'frequencies', 'tokenization', 'and', 'spelling', 'correction', 'is', 'used', 'to', 'add', 'on', 'top', 'of', 'a', 'method', 'to', 'send', 'data']\n"
     ]
    }
   ],
   "source": [
    "for word_tokens in blob.sentences:\n",
    "    print(word_tokens.words) # word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0d1ecba9-a269-407d-bf8e-46efb11af89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'TextBlob']\n",
      "['TextBlob', 'library']\n",
      "['library', 'is']\n",
      "['is', 'used']\n",
      "['used', 'for']\n",
      "['for', 'processing']\n",
      "['processing', 'textual']\n",
      "['textual', 'data']\n",
      "['data', 'TextBlob']\n",
      "['TextBlob', 'is']\n",
      "['is', 'a']\n",
      "['a', 'Python']\n",
      "['Python', '2']\n",
      "['2', 'and']\n",
      "['and', '3']\n",
      "['3', 'library']\n",
      "['library', 'that']\n",
      "['that', 'provides']\n",
      "['provides', 'a']\n",
      "['a', 'simple']\n",
      "['simple', 'API']\n",
      "['API', 'for']\n",
      "['for', 'diving']\n",
      "['diving', 'into']\n",
      "['into', 'common']\n",
      "['common', 'natural']\n",
      "['natural', 'language']\n",
      "['language', 'processing']\n",
      "['processing', 'NLP']\n",
      "['NLP', 'tasks']\n",
      "['tasks', 'The']\n",
      "['The', 'features']\n",
      "['features', 'of']\n",
      "['of', 'the']\n",
      "['the', 'TextBlob']\n",
      "['TextBlob', 'library']\n",
      "['library', 'include']\n",
      "['include', 'parts-of-speech']\n",
      "['parts-of-speech', 'tagging']\n",
      "['tagging', 'word']\n",
      "['word', 'inflection']\n",
      "['inflection', 'word']\n",
      "['word', 'and']\n",
      "['and', 'phrase']\n",
      "['phrase', 'frequencies']\n",
      "['frequencies', 'tokenization']\n",
      "['tokenization', 'and']\n",
      "['and', 'spelling']\n",
      "['spelling', 'correction']\n",
      "['correction', 'is']\n",
      "['is', 'used']\n",
      "['used', 'to']\n",
      "['to', 'add']\n",
      "['add', 'on']\n",
      "['on', 'top']\n",
      "['top', 'of']\n",
      "['of', 'a']\n",
      "['a', 'method']\n",
      "['method', 'to']\n",
      "['to', 'send']\n",
      "['send', 'data']\n"
     ]
    }
   ],
   "source": [
    "for bigram in blob.ngrams(2):\n",
    "    print(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7e0700b3-ce04-4839-bd28-8109bb101881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'TextBlob', 'library', 'is', 'used']\n",
      "['TextBlob', 'library', 'is', 'used', 'for']\n",
      "['library', 'is', 'used', 'for', 'processing']\n",
      "['is', 'used', 'for', 'processing', 'textual']\n",
      "['used', 'for', 'processing', 'textual', 'data']\n",
      "['for', 'processing', 'textual', 'data', 'TextBlob']\n",
      "['processing', 'textual', 'data', 'TextBlob', 'is']\n",
      "['textual', 'data', 'TextBlob', 'is', 'a']\n",
      "['data', 'TextBlob', 'is', 'a', 'Python']\n",
      "['TextBlob', 'is', 'a', 'Python', '2']\n",
      "['is', 'a', 'Python', '2', 'and']\n",
      "['a', 'Python', '2', 'and', '3']\n",
      "['Python', '2', 'and', '3', 'library']\n",
      "['2', 'and', '3', 'library', 'that']\n",
      "['and', '3', 'library', 'that', 'provides']\n",
      "['3', 'library', 'that', 'provides', 'a']\n",
      "['library', 'that', 'provides', 'a', 'simple']\n",
      "['that', 'provides', 'a', 'simple', 'API']\n",
      "['provides', 'a', 'simple', 'API', 'for']\n",
      "['a', 'simple', 'API', 'for', 'diving']\n",
      "['simple', 'API', 'for', 'diving', 'into']\n",
      "['API', 'for', 'diving', 'into', 'common']\n",
      "['for', 'diving', 'into', 'common', 'natural']\n",
      "['diving', 'into', 'common', 'natural', 'language']\n",
      "['into', 'common', 'natural', 'language', 'processing']\n",
      "['common', 'natural', 'language', 'processing', 'NLP']\n",
      "['natural', 'language', 'processing', 'NLP', 'tasks']\n",
      "['language', 'processing', 'NLP', 'tasks', 'The']\n",
      "['processing', 'NLP', 'tasks', 'The', 'features']\n",
      "['NLP', 'tasks', 'The', 'features', 'of']\n",
      "['tasks', 'The', 'features', 'of', 'the']\n",
      "['The', 'features', 'of', 'the', 'TextBlob']\n",
      "['features', 'of', 'the', 'TextBlob', 'library']\n",
      "['of', 'the', 'TextBlob', 'library', 'include']\n",
      "['the', 'TextBlob', 'library', 'include', 'parts-of-speech']\n",
      "['TextBlob', 'library', 'include', 'parts-of-speech', 'tagging']\n",
      "['library', 'include', 'parts-of-speech', 'tagging', 'word']\n",
      "['include', 'parts-of-speech', 'tagging', 'word', 'inflection']\n",
      "['parts-of-speech', 'tagging', 'word', 'inflection', 'word']\n",
      "['tagging', 'word', 'inflection', 'word', 'and']\n",
      "['word', 'inflection', 'word', 'and', 'phrase']\n",
      "['inflection', 'word', 'and', 'phrase', 'frequencies']\n",
      "['word', 'and', 'phrase', 'frequencies', 'tokenization']\n",
      "['and', 'phrase', 'frequencies', 'tokenization', 'and']\n",
      "['phrase', 'frequencies', 'tokenization', 'and', 'spelling']\n",
      "['frequencies', 'tokenization', 'and', 'spelling', 'correction']\n",
      "['tokenization', 'and', 'spelling', 'correction', 'is']\n",
      "['and', 'spelling', 'correction', 'is', 'used']\n",
      "['spelling', 'correction', 'is', 'used', 'to']\n",
      "['correction', 'is', 'used', 'to', 'add']\n",
      "['is', 'used', 'to', 'add', 'on']\n",
      "['used', 'to', 'add', 'on', 'top']\n",
      "['to', 'add', 'on', 'top', 'of']\n",
      "['add', 'on', 'top', 'of', 'a']\n",
      "['on', 'top', 'of', 'a', 'method']\n",
      "['top', 'of', 'a', 'method', 'to']\n",
      "['of', 'a', 'method', 'to', 'send']\n",
      "['a', 'method', 'to', 'send', 'data']\n"
     ]
    }
   ],
   "source": [
    "for ngram in blob.ngrams(5):\n",
    "    print(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "90c7be27-9a5c-4911-9a82-99aed5afc312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 10.2/981.5 kB ? eta -:--:--\n",
      "     - ----------------------------------- 41.0/981.5 kB 653.6 kB/s eta 0:00:02\n",
      "     ---- --------------------------------- 122.9/981.5 kB 1.2 MB/s eta 0:00:01\n",
      "     ----------- -------------------------- 286.7/981.5 kB 1.8 MB/s eta 0:00:01\n",
      "     ------------- ------------------------ 348.2/981.5 kB 1.7 MB/s eta 0:00:01\n",
      "     ----------------- -------------------- 450.6/981.5 kB 1.9 MB/s eta 0:00:01\n",
      "     -------------------- ----------------- 522.2/981.5 kB 1.9 MB/s eta 0:00:01\n",
      "     -------------------------- ----------- 675.8/981.5 kB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------------- ------ 819.2/981.5 kB 2.1 MB/s eta 0:00:01\n",
      "     -------------------------------------  972.8/981.5 kB 2.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- 981.5/981.5 kB 2.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six in c:\\users\\udaya\\anaconda3\\lib\\site-packages (from langdetect) (1.16.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993253 sha256=c02047082c44b07437fe4214460875b81dd3f1eac046affcf6b75424fa72d357\n",
      "  Stored in directory: c:\\users\\udaya\\appdata\\local\\pip\\cache\\wheels\\0a\\f2\\b2\\e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f4483947-ddd4-453a-9214-1d2c8498656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6f959ec2-4399-4167-96d3-7392e07f3a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Hi, How are you doing? - and how you been all these days?\")"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(\"Hi, How are you doing? - and how you been all these days?\")\n",
    "blob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb81d42e-1ba2-487b-8853-5c896f84a30d",
   "metadata": {},
   "source": [
    "# PoS - Parts of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "23ba1085-a538-49ea-8cf3-7deb90c24730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The TextBlob library is used for processing textual data. \\nTextBlob is a Python (2 and 3) library that provides a simple API for diving into common natural language processing (NLP) tasks.\\nThe features of the TextBlob library include parts-of-speech tagging, word inflection, word and phrase frequencies, tokenization, and \\nspelling correction is used to add on top of a method to send data.'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1b324c96-3e59-4094-b727-1feb81e4d4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_blob = TextBlob(mytxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6c545734-14a7-44a3-8350-b20ca8259b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"The TextBlob library is used for processing textual data. \n",
       "TextBlob is a Python (2 and 3) library that provides a simple API for diving into common natural language processing (NLP) tasks.\n",
       "The features of the TextBlob library include parts-of-speech tagging, word inflection, word and phrase frequencies, tokenization, and \n",
       "spelling correction is used to add on top of a method to send data.\")"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "65428ccd-fbf4-490e-9498-6b70ca5a6e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('TextBlob', 'NNP'),\n",
       " ('library', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('used', 'VBN'),\n",
       " ('for', 'IN'),\n",
       " ('processing', 'VBG'),\n",
       " ('textual', 'JJ'),\n",
       " ('data', 'NNS'),\n",
       " ('TextBlob', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('Python', 'NNP'),\n",
       " ('2', 'CD'),\n",
       " ('and', 'CC'),\n",
       " ('3', 'CD'),\n",
       " ('library', 'NN'),\n",
       " ('that', 'WDT'),\n",
       " ('provides', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('simple', 'JJ'),\n",
       " ('API', 'NNP'),\n",
       " ('for', 'IN'),\n",
       " ('diving', 'VBG'),\n",
       " ('into', 'IN'),\n",
       " ('common', 'JJ'),\n",
       " ('natural', 'JJ'),\n",
       " ('language', 'NN'),\n",
       " ('processing', 'NN'),\n",
       " ('NLP', 'NNP'),\n",
       " ('tasks', 'NNS'),\n",
       " ('The', 'DT'),\n",
       " ('features', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('TextBlob', 'NNP'),\n",
       " ('library', 'JJ'),\n",
       " ('include', 'VBP'),\n",
       " ('parts-of-speech', 'JJ'),\n",
       " ('tagging', 'NN'),\n",
       " ('word', 'NN'),\n",
       " ('inflection', 'NN'),\n",
       " ('word', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('phrase', 'NN'),\n",
       " ('frequencies', 'NNS'),\n",
       " ('tokenization', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('spelling', 'VBG'),\n",
       " ('correction', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('used', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('add', 'VB'),\n",
       " ('on', 'IN'),\n",
       " ('top', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('method', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('send', 'VB'),\n",
       " ('data', 'NNS')]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_blob.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e5897cbd-5dc4-413b-95e5-849a1e400b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('TextBlob', 'NNP'),\n",
       " ('library', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('used', 'VBN'),\n",
       " ('for', 'IN'),\n",
       " ('processing', 'VBG'),\n",
       " ('textual', 'JJ'),\n",
       " ('data', 'NNS'),\n",
       " ('TextBlob', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('Python', 'NNP'),\n",
       " ('2', 'CD'),\n",
       " ('and', 'CC'),\n",
       " ('3', 'CD'),\n",
       " ('library', 'NN'),\n",
       " ('that', 'WDT'),\n",
       " ('provides', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('simple', 'JJ'),\n",
       " ('API', 'NNP'),\n",
       " ('for', 'IN'),\n",
       " ('diving', 'VBG'),\n",
       " ('into', 'IN'),\n",
       " ('common', 'JJ'),\n",
       " ('natural', 'JJ'),\n",
       " ('language', 'NN'),\n",
       " ('processing', 'NN'),\n",
       " ('NLP', 'NNP'),\n",
       " ('tasks', 'NNS'),\n",
       " ('The', 'DT'),\n",
       " ('features', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('TextBlob', 'NNP'),\n",
       " ('library', 'JJ'),\n",
       " ('include', 'VBP'),\n",
       " ('parts-of-speech', 'JJ'),\n",
       " ('tagging', 'NN'),\n",
       " ('word', 'NN'),\n",
       " ('inflection', 'NN'),\n",
       " ('word', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('phrase', 'NN'),\n",
       " ('frequencies', 'NNS'),\n",
       " ('tokenization', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('spelling', 'VBG'),\n",
       " ('correction', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('used', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('add', 'VB'),\n",
       " ('on', 'IN'),\n",
       " ('top', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('method', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('send', 'VB'),\n",
       " ('data', 'NNS')]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_blob.pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "25248e6f-0a6c-404a-b8bc-22cad2b2925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_blob2 = TextBlob(\"Google is great erach engine to search almost anything on internet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "99c0aed0-c5c6-4e19-b020-8328197ba531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textblob.blob.TextBlob"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pos_blob2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2f12ed5e-343a-465b-818f-cec8681a4166",
   "metadata": {},
   "outputs": [
    {
     "ename": "MissingCorpusError",
     "evalue": "\nLooks like you are missing some required data for this feature.\n\nTo download the necessary data, simply run\n\n    python -m textblob.download_corpora\n\nor use the NLTK downloader to download the missing data: http://nltk.org/data.html\nIf this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mbrown\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('brown')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/brown.zip/brown/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\udaya/nltk_data'\n    - 'C:\\\\Users\\\\udaya\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\udaya\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\udaya\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\udaya\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\textblob\\decorators.py:35\u001b[0m, in \u001b[0;36mrequires_nltk_corpus.<locals>.decorated\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\textblob\\en\\np_extractors.py:113\u001b[0m, in \u001b[0;36mFastNPExtractor.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;129m@requires_nltk_corpus\u001b[39m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 113\u001b[0m     train_data \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mcorpus\u001b[38;5;241m.\u001b[39mbrown\u001b[38;5;241m.\u001b[39mtagged_sents(categories\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnews\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    114\u001b[0m     regexp_tagger \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mRegexpTagger(\n\u001b[0;32m    115\u001b[0m         [\n\u001b[0;32m    116\u001b[0m             (\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^-?[0-9]+(.[0-9]+)?$\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCD\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    128\u001b[0m         ]\n\u001b[0;32m    129\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__load()\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m     root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mbrown\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('brown')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/brown\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\udaya/nltk_data'\n    - 'C:\\\\Users\\\\udaya\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\udaya\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\udaya\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\udaya\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMissingCorpusError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[153], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m np \u001b[38;5;129;01min\u001b[39;00m pos_blob\u001b[38;5;241m.\u001b[39mnoun_phrases:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(np)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\textblob\\decorators.py:23\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m---> 23\u001b[0m value \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(obj)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\textblob\\blob.py:477\u001b[0m, in \u001b[0;36mBaseBlob.noun_phrases\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;129m@cached_property\u001b[39m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnoun_phrases\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    473\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a list of noun phrases for this blob.\"\"\"\u001b[39;00m\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m WordList(\n\u001b[0;32m    475\u001b[0m         [\n\u001b[0;32m    476\u001b[0m             phrase\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m--> 477\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m phrase \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnp_extractor\u001b[38;5;241m.\u001b[39mextract(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw)\n\u001b[0;32m    478\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(phrase) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    479\u001b[0m         ]\n\u001b[0;32m    480\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\textblob\\en\\np_extractors.py:143\u001b[0m, in \u001b[0;36mFastNPExtractor.extract\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of noun phrases (strings) for body of text.\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trained:\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    144\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenize_sentence(sentence)\n\u001b[0;32m    145\u001b[0m tagged \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtagger\u001b[38;5;241m.\u001b[39mtag(tokens)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\textblob\\decorators.py:37\u001b[0m, in \u001b[0;36mrequires_nltk_corpus.<locals>.decorated\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m---> 37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MissingCorpusError() \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m\n",
      "\u001b[1;31mMissingCorpusError\u001b[0m: \nLooks like you are missing some required data for this feature.\n\nTo download the necessary data, simply run\n\n    python -m textblob.download_corpora\n\nor use the NLTK downloader to download the missing data: http://nltk.org/data.html\nIf this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.\n"
     ]
    }
   ],
   "source": [
    "for np in pos_blob.nou:\n",
    "    print(np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "5f6765e2-de3e-45b7-bb09-b734191bcafc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Hi, How are you doing? - and how you been all these days?\")"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "3530b7b4-37ca-4aac-a024-4e23b29143ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hi', 'NNP'),\n",
       " ('How', 'NNP'),\n",
       " ('are', 'VBP'),\n",
       " ('you', 'PRP'),\n",
       " ('doing', 'VBG'),\n",
       " ('and', 'CC'),\n",
       " ('how', 'WRB'),\n",
       " ('you', 'PRP'),\n",
       " ('been', 'VBN'),\n",
       " ('all', 'PDT'),\n",
       " ('these', 'DT'),\n",
       " ('days', 'NNS')]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "3775217e-af86-4ac8-9c46-463d7aedd992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are => VBP\n",
      "doing => VBG\n",
      "been => VBN\n"
     ]
    }
   ],
   "source": [
    "for word, verb in blob.tags:\n",
    "    if verb in ['VB', 'VBZ', 'VBP', 'VBD', 'VBN', 'VBG']:\n",
    "        print(f'{word} => {verb}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1589b2b-7138-419d-ba9c-3d7356863442",
   "metadata": {},
   "source": [
    "# Word Inflection - <h3> Word formation by adding to the root/base of the word </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7007907f-7cc0-4b95-af75-68f81ae2351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytree = [\"study\", \"studying\", \"student\", \"studies\", \"studious\"]\n",
    "myblog = [\"blog\", \"blogging\", \"logging\", \"bloger\", \"blogs\",\"vlogs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb5ee245-a97f-4c21-97bb-54a66dac4322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob, Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67b9c545-ffed-4bca-a331-9f2dcc382565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pens'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = Word(\"pens\")\n",
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5195a0d9-dd87-423a-abb8-28353a8aa0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pen'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = Word('Pens').singularize()\n",
    "#s.singularize()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "06e666b3-7b1a-4f0f-b127-1cb0ed11c902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex1 = Word('running')\n",
    "ex1.lemmatize('v') #verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "de31dd72-6bb5-4afd-9bcb-060b5be4718a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'radiis'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "w = Word(\"radiis\")\n",
    "w.lemmatize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a2e1eb5b-94fe-4af1-a87e-44c10b6520fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Father and Son\")"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = TextBlob(\"Father\")\n",
    "b = TextBlob(\"Son\")\n",
    "a + ' and ' + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "644c3fbd-6121-4d49-93f2-714e4d267fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text=\"Natural Language Processing (NLP) is a field of Computer Science, Artificial Intelligence, and Computational Linguistics concerned with the interactions between Computers and Human (natural) Languages, and, in particular, concerned with Programming Computers to fruitfully process large natural language corpora. Challenges in Natural Language Processing frequently involve Natural Language Understanding, Natural Language Generation frequently from formal, machine-readable logical forms), connecting language and machine perception, managing human-computer dialog systems, or some combination thereof.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c65c07eb-de44-4510-a3ee-4bb3a65de595",
   "metadata": {},
   "outputs": [],
   "source": [
    "object = TextBlob(Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "21faff81-336f-471a-a692-87bf656c84a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Natural Language Processing (NLP) is a field of Computer Science, Artificial Intelligence, and Computational Linguistics concerned with the interactions between Computers and Human (natural) Languages, and, in particular, concerned with Programming Computers to fruitfully process large natural language corpora. Challenges in Natural Language Processing frequently involve Natural Language Understanding, Natural Language Generation frequently from formal, machine-readable logical forms), connecting language and machine perception, managing human-computer dialog systems, or some combination thereof.\")"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "32b2982c-ad92-4637-ba9c-e35757000d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sentence(\"Natural Language Processing (NLP) is a field of Computer Science, Artificial Intelligence, and Computational Linguistics concerned with the interactions between Computers and Human (natural) Languages, and, in particular, concerned with Programming Computers to fruitfully process large natural language corpora.\"), Sentence(\"Challenges in Natural Language Processing frequently involve Natural Language Understanding, Natural Language Generation frequently from formal, machine-readable logical forms), connecting language and machine perception, managing human-computer dialog systems, or some combination thereof.\")]\n"
     ]
    }
   ],
   "source": [
    "print(object.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6cc71921-1295-4cb9-a090-c955594aab34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"I am not a teachers, I am a student's\")"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sentence = TextBlob(\"I am not a teachers, I am a student's\")\n",
    "my_sentence.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1f54313c-a9a3-4e65-aed3-bd1cf0740595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Words = TextBlob(\"Betty Bob bought some butter. But she said the Butterâ€™s bitter. If I put it in my batter, it will make my batter bitter. But a bit of better butter will make my batter better.\")\n",
    "Words.word_counts['butter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "339c6650-4af4-439f-993a-6e516db34304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'find'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex2  = Word('finding')\n",
    "ex2.lemmatize('v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "54aa5f05-e4d3-4256-804e-2a0093d325d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['study', 'studying', 'student', 'studies', 'studious']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "73dc7dcc-913f-4ce3-a85d-60fba0798677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "study => study\n",
      "studying => studying\n",
      "student => student\n",
      "studies => study\n",
      "studious => studious\n"
     ]
    }
   ],
   "source": [
    "for i in mytree:\n",
    "    result = Word(i).lemmatize('n')\n",
    "    print(f'{i} => {result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ad4f14b4-a434-4a49-b8ac-0e383313d3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blog => blog\n",
      "blogging => blogging\n",
      "logging => log\n",
      "bloger => bloger\n",
      "blogs => blog\n",
      "vlogs => vlogs\n"
     ]
    }
   ],
   "source": [
    "for i in myblog:\n",
    "    result = Word(i).lemmatize('v')\n",
    "    print(f'{i} => {result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba056b16-9267-4155-9d13-2d65dabf3069",
   "metadata": {},
   "source": [
    "# Sentiment Analasys with TextBlob\n",
    "- openion mining or Emotion AI\n",
    "- categorizing openions and attitude expressed  in a piece of text(positive, negetive, nuetral)\n",
    "- determining the attitute of a speaker or writer with respect to the topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f1b0c527-3674-4a8d-8bbc-14b18a9d1f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "myfeelings = [\"i love my phone but would not reccomend it to any of my friends\",\n",
    "              \"I love this watch\", \"This is an amazing library\", \n",
    "              \"I do not like this restaurent\", \"wow what a tip\", \n",
    "              \"surprizingly a nice movie\",\n",
    "              \"hard to resist\", \n",
    "              \"one lousy film\", \n",
    "              \"this is too long\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a6142f0b-36ae-4769-b406-bee12e7db42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fd96088c-65c4-469b-9c04-70a9d72831d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"I love this watch\")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review1 = TextBlob(\"I love this watch\")\n",
    "review1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "004a16b3-9ea1-4a9e-97ad-109a74e35d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.5, subjectivity=0.6)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review1.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a223ebee-a05a-4227-98ce-d06b2f1c101c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=-0.8, subjectivity=0.9)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review2  = TextBlob(\"I hate this wrist watch\")\n",
    "review2.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ecaeb342-11d0-4888-93fa-e69af289af40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.8"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review2.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d248c3bd-778e-45f5-961a-ce0ebac1dcb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review2.sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6f5cd34c-cf04-406b-801a-53f33433066e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i love my phone but would not reccomend it to any of my friends',\n",
       " 'I love this watch',\n",
       " 'This is an amazing livrary',\n",
       " 'I do not like this restaurent',\n",
       " 'wow what a tip',\n",
       " 'surprizingly a nice movie',\n",
       " 'hard to resist',\n",
       " 'one lousy film',\n",
       " 'this is too long']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myfeelings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "866593cc-7a19-4cf0-8b89-295ac19e1cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love my phone but would not reccomend it to any of my friends => polarity: 0.5\n",
      "I love this watch => polarity: 0.5\n",
      "This is an amazing library => polarity: 0.6000000000000001\n",
      "I do not like this restaurent => polarity: 0.0\n",
      "wow what a tip => polarity: 0.1\n",
      "surprizingly a nice movie => polarity: 0.6\n",
      "hard to resist => polarity: -0.2916666666666667\n",
      "one lousy film => polarity: -0.5\n",
      "this is too long => polarity: -0.05\n"
     ]
    }
   ],
   "source": [
    "for sent in myfeelings:\n",
    "    result = TextBlob(sent).sentiment.polarity\n",
    "    print(f'{sent} => polarity: {result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1440ad-0c96-4cf3-ab4c-f4d128e36327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ab98a3-6f58-4101-874b-f2701328a47e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316ac29c-6aa9-4f75-9e05-dfb4f67f179c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c83f4c-5d24-4173-afa2-47c41ee22398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69c151b-dda7-490d-b7a5-80f6a3e8958f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6a6e27-aeca-43c0-8afc-4514fcb71d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973b6d41-77b3-4d2e-a409-fcf80ae87ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c78305c-427e-448f-a85f-6292b44219a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01940136-8087-422c-819a-a4dc04c1ccbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
