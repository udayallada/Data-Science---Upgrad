{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d8b7e5f-ddba-43d3-8a40-7fec84fba224",
   "metadata": {},
   "source": [
    "# re - Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22ac389c-d6d8-443c-a9aa-4d2927adb972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b80ed79-8807-47aa-98f2-80dc4c8180d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_test(sentence):\n",
    "    return re.sub(r'([^\\s\\w]|_)+', ' ', sentence).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d54f69a1-83ad-4b14-9b74-f8e0571fcbed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Regular expressions are powerful for pattern matching, text parsing, \\nand tokenization, making them a valuable tool when dealing with text data.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = '''Regular expressions are powerful for pattern matching, text parsing, \n",
    "and tokenization, making them a valuable tool when dealing with text data.'''\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23791ca7-f888-4dca-b659-71ab8b7366f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Regular',\n",
       " 'expressions',\n",
       " 'are',\n",
       " 'powerful',\n",
       " 'for',\n",
       " 'pattern',\n",
       " 'matching',\n",
       " 'text',\n",
       " 'parsing',\n",
       " 'and',\n",
       " 'tokenization',\n",
       " 'making',\n",
       " 'them',\n",
       " 'a',\n",
       " 'valuable',\n",
       " 'tool',\n",
       " 'when',\n",
       " 'dealing',\n",
       " 'with',\n",
       " 'text',\n",
       " 'data']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_test(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e613241-70d8-46e5-8aad-37ec68b1b883",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2 = clean_test(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f129b735-f5d1-4270-9751-47720901636c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Regular',\n",
       " 'expressions',\n",
       " 'are',\n",
       " 'powerful',\n",
       " 'for',\n",
       " 'pattern',\n",
       " 'matching',\n",
       " 'text',\n",
       " 'parsing',\n",
       " 'and',\n",
       " 'tokenization',\n",
       " 'making',\n",
       " 'them',\n",
       " 'a',\n",
       " 'valuable',\n",
       " 'tool',\n",
       " 'when',\n",
       " 'dealing',\n",
       " 'with',\n",
       " 'text',\n",
       " 'data']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9188a696-7ca6-49a6-933c-b807f180d98c",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0e86fe36-1d81-406c-a2c5-39761b4842ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram_extractor(sentence, n):\n",
    "    tokens = re.sub(r'([^\\s\\w]|_)+', ' ', sentence).split()\n",
    "    for i in range(len(tokens)-n+1):\n",
    "        print(tokens[i:i+n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9821afd6-22f2-4e97-8a9b-184bc184cde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Regular', 'expressions', 'are', 'powerful', 'for']\n",
      "['expressions', 'are', 'powerful', 'for', 'pattern']\n",
      "['are', 'powerful', 'for', 'pattern', 'matching']\n",
      "['powerful', 'for', 'pattern', 'matching', 'text']\n",
      "['for', 'pattern', 'matching', 'text', 'parsing']\n",
      "['pattern', 'matching', 'text', 'parsing', 'and']\n",
      "['matching', 'text', 'parsing', 'and', 'tokenization']\n",
      "['text', 'parsing', 'and', 'tokenization', 'making']\n",
      "['parsing', 'and', 'tokenization', 'making', 'them']\n",
      "['and', 'tokenization', 'making', 'them', 'a']\n",
      "['tokenization', 'making', 'them', 'a', 'valuable']\n",
      "['making', 'them', 'a', 'valuable', 'tool']\n",
      "['them', 'a', 'valuable', 'tool', 'when']\n",
      "['a', 'valuable', 'tool', 'when', 'dealing']\n",
      "['valuable', 'tool', 'when', 'dealing', 'with']\n",
      "['tool', 'when', 'dealing', 'with', 'text']\n",
      "['when', 'dealing', 'with', 'text', 'data']\n"
     ]
    }
   ],
   "source": [
    "n_gram_extractor(sentence,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dcf256fd-dae5-410a-aced-1722a6b33788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Regular', 'expressions', 'are']\n",
      "['expressions', 'are', 'powerful']\n",
      "['are', 'powerful', 'for']\n",
      "['powerful', 'for', 'pattern']\n",
      "['for', 'pattern', 'matching']\n",
      "['pattern', 'matching', 'text']\n",
      "['matching', 'text', 'parsing']\n",
      "['text', 'parsing', 'and']\n",
      "['parsing', 'and', 'tokenization']\n",
      "['and', 'tokenization', 'making']\n",
      "['tokenization', 'making', 'them']\n",
      "['making', 'them', 'a']\n",
      "['them', 'a', 'valuable']\n",
      "['a', 'valuable', 'tool']\n",
      "['valuable', 'tool', 'when']\n",
      "['tool', 'when', 'dealing']\n",
      "['when', 'dealing', 'with']\n",
      "['dealing', 'with', 'text']\n",
      "['with', 'text', 'data']\n"
     ]
    }
   ],
   "source": [
    "n_gram_extractor(sentence, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "506e2d3a-1834-4058-91e3-26bb443bf7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aaa4ecaf-1d97-43c0-bbd6-a50e2efb4085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Regular', 'expressions'),\n",
       " ('expressions', 'are'),\n",
       " ('are', 'powerful'),\n",
       " ('powerful', 'for'),\n",
       " ('for', 'pattern'),\n",
       " ('pattern', 'matching,'),\n",
       " ('matching,', 'text'),\n",
       " ('text', 'parsing,'),\n",
       " ('parsing,', 'and'),\n",
       " ('and', 'tokenization,'),\n",
       " ('tokenization,', 'making'),\n",
       " ('making', 'them'),\n",
       " ('them', 'a'),\n",
       " ('a', 'valuable'),\n",
       " ('valuable', 'tool'),\n",
       " ('tool', 'when'),\n",
       " ('when', 'dealing'),\n",
       " ('dealing', 'with'),\n",
       " ('with', 'text'),\n",
       " ('text', 'data.')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(sentence.split(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "493125fc-a3a4-4bcf-8ad7-22c69b9d1218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Regular', 'expressions', 'are', 'powerful', 'for'),\n",
       " ('expressions', 'are', 'powerful', 'for', 'pattern'),\n",
       " ('are', 'powerful', 'for', 'pattern', 'matching,'),\n",
       " ('powerful', 'for', 'pattern', 'matching,', 'text'),\n",
       " ('for', 'pattern', 'matching,', 'text', 'parsing,'),\n",
       " ('pattern', 'matching,', 'text', 'parsing,', 'and'),\n",
       " ('matching,', 'text', 'parsing,', 'and', 'tokenization,'),\n",
       " ('text', 'parsing,', 'and', 'tokenization,', 'making'),\n",
       " ('parsing,', 'and', 'tokenization,', 'making', 'them'),\n",
       " ('and', 'tokenization,', 'making', 'them', 'a'),\n",
       " ('tokenization,', 'making', 'them', 'a', 'valuable'),\n",
       " ('making', 'them', 'a', 'valuable', 'tool'),\n",
       " ('them', 'a', 'valuable', 'tool', 'when'),\n",
       " ('a', 'valuable', 'tool', 'when', 'dealing'),\n",
       " ('valuable', 'tool', 'when', 'dealing', 'with'),\n",
       " ('tool', 'when', 'dealing', 'with', 'text'),\n",
       " ('when', 'dealing', 'with', 'text', 'data.')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(sentence.split(),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5f6d68d1-85ea-43b8-9fe7-c4f14c9712cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Regular', 'expressions'),\n",
       " ('expressions', 'are'),\n",
       " ('are', 'powerful'),\n",
       " ('powerful', 'for'),\n",
       " ('for', 'pattern'),\n",
       " ('pattern', 'matching,'),\n",
       " ('matching,', 'text'),\n",
       " ('text', 'parsing,'),\n",
       " ('parsing,', 'and'),\n",
       " ('and', 'tokenization,'),\n",
       " ('tokenization,', 'making'),\n",
       " ('making', 'them'),\n",
       " ('them', 'a'),\n",
       " ('a', 'valuable'),\n",
       " ('valuable', 'tool'),\n",
       " ('tool', 'when'),\n",
       " ('when', 'dealing'),\n",
       " ('dealing', 'with'),\n",
       " ('with', 'text'),\n",
       " ('text', 'data.')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(sentence.split(),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55eb7d26-d5aa-4b5e-aedf-ccd49525c716",
   "metadata": {},
   "source": [
    "# Feature Extraction methods\n",
    "\n",
    "- Bag of words\n",
    "- Distributed Representation\n",
    "- Word Embeddings (Word2Vec, GloVe, FastText)\n",
    "- TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "- N-grams\n",
    "- Part-of-Speech (POS) Tagging\n",
    "- Named Entity Recognition (NER)\n",
    "- Topic Modeling (LDA)\n",
    "- Dependency Parsing\n",
    "- Frequency vector\n",
    "- one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d140f297-2a85-47e8-b533-5be495e902db",
   "metadata": {},
   "source": [
    "# Tokenizers - Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "03b95972-6119-4e54-b0c2-b96966f8e39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "sentence = '''Love to learn NLP(Natural Programming Language) in 2024 for career advancement in AI!!!'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "085ad685-1dd0-43fa-a289-a1a04dba4f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_tweektokenizer(text):\n",
    "    tweet_tokenizer = TweetTokenizer()\n",
    "    return tweet_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4905cde7-9a22-4484-af1c-a231545ff0cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Love',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'NLP',\n",
       " 'in',\n",
       " '2024',\n",
       " 'for',\n",
       " 'career',\n",
       " 'advancement',\n",
       " 'in',\n",
       " 'AI',\n",
       " '!',\n",
       " '!',\n",
       " '!']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_with_tweektokenizer(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ba26488e-2433-4cc8-adf6-051c78793431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_With_mwe(text):\n",
    "    mwe_tokenize = MWETokenizer([('Republic', 'Day')])\n",
    "    mwe_tokenize.add_mwe(('Indian', 'Army'))\n",
    "    return mwe_tokenize.tokenize(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "480098f2-198b-4e8c-9a2d-f5f5306b6f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Love',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'NLP',\n",
       " 'in',\n",
       " '2024',\n",
       " 'for',\n",
       " 'career',\n",
       " 'advancement',\n",
       " 'in',\n",
       " 'AI!!!']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_With_mwe(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ab5e40d0-8103-458d-92a7-10e3b9b9d6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_regexp(text):\n",
    "    reg_tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\s+')\n",
    "    return reg_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d7f084de-0ab9-41f2-87d3-85f3b6331780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " 'Love',\n",
       " ' ',\n",
       " 'to',\n",
       " ' ',\n",
       " 'learn',\n",
       " ' ',\n",
       " 'NLP',\n",
       " ' ',\n",
       " 'in',\n",
       " ' ',\n",
       " '2024',\n",
       " ' ',\n",
       " 'for',\n",
       " ' ',\n",
       " 'career',\n",
       " ' ',\n",
       " 'advancement',\n",
       " ' ',\n",
       " 'in',\n",
       " ' ',\n",
       " 'AI']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_with_regexp(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3a16728f-3ee8-4cdd-b06e-6b382b1d81a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_wst(text):\n",
    "    wst_tokenizer = WhitespaceTokenizer()\n",
    "    return wst_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "165046d0-3151-468c-a655-222bfb9cd21c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Love',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'NLP',\n",
       " 'in',\n",
       " '2024',\n",
       " 'for',\n",
       " 'career',\n",
       " 'advancement',\n",
       " 'in',\n",
       " 'AI!!!']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_with_wst(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "af73157c-d9d2-426e-963a-670b194cf6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_wordpunct(text):\n",
    "    wordpunct_tokenizer = WordPunctTokenizer()\n",
    "    return wordpunct_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b8eba6e0-bc62-4439-ac64-ea145502b549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Love',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'NLP',\n",
       " 'in',\n",
       " '2024',\n",
       " 'for',\n",
       " 'career',\n",
       " 'advancement',\n",
       " 'in',\n",
       " 'AI',\n",
       " '!!!']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_with_wordpunct(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "66e33772-62e4-49e6-9e84-858603d59fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenize_with_tweektokenizer(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1773dfa0-11d4-40c3-95a5-3de9f4844bc6",
   "metadata": {},
   "source": [
    "# stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "758277ba-88b1-437c-aea4-dc59f1c0a20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f851b76c-29d7-4487-b957-656ae63038e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Love to learn NLP(Natural Programming Language) in 2024 for career advancement in AI!!!'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "860b6b07-d715-4cdf-8a81-a2763efc5e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stemms(text):\n",
    "    reg_stem = RegexpStemmer('ing$', min=4)\n",
    "    #any string ending with regex 'ing' will be removed. below code will return by removing the ing in the sentence.\n",
    "    return ' '.join(reg_stem.stem(wd) for wd in text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "096b5ffb-6f7c-40d5-a47f-ba19b8365c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Love to learn NLP(Natural Programm Language) in 2024 for career advancement in AI!!!'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stemms(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "bb3fc176-df9a-4b09-a04b-287b6d5878a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "cf20e73e-0f80-43ae-928c-989c2aaff43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stemms(text):\n",
    "    port_stemm = PorterStemmer()\n",
    "    return ' '.join(port_stemm.stem(wd) for wd in text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "827485de-e59b-4c73-bffe-a904eb0a557e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love to learn nlp(natur program language) in 2024 for career advanc in ai!!!'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stemms(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "546c1c5c-d1a7-4a49-92a7-40140707b0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\udaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "14b871ea-b31d-4a0e-bd0c-5b91d19cf512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Love to learn NLP(Natural Programming Language) in 2024 for career advancement in AI!!!'"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "497da35b-8c65-46e9-882a-c4ee1d47aca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmitizer = WordNetLemmatizer()\n",
    "def get_lemmas(text):\n",
    "    return ' '.join(lemmitizer.lemmatize(word) for word in word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "6e6d54b3-b942-4361-b8bf-c20708867a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Love to learn NLP ( Natural Programming Language ) in 2024 for career advancement in AI ! ! !'"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_lemmas(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "8795cab4-ca1d-49c7-9bb4-fe231ea70723",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\udaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import download\n",
    "download('stopwords')\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "95fb2b8a-82c0-4e49-80bc-6382a6c5e53c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "ac3dcdea-47f7-4553-9764-f3806e6c3d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text, stop_words):\n",
    "    return ' '.join([word for word in word_tokenize(text) if word.lower() not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "203dbe61-3c2a-4e30-84b1-bf3946c2a957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(remove_stop_words(sentence, stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "298dac34-e9c6-4149-9c77-5d35edc5754c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Love learn NLP ( Natural Programming Language ) 2024 career advancement AI ! ! !'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stop_words(sentence, stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96aa54a6-0a99-415b-a79e-2662e75cb03d",
   "metadata": {},
   "source": [
    "# Vectorization - Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d49804-cd80-4060-8b8c-af9ca257fe57",
   "metadata": {},
   "source": [
    "<h3> Feature extraction </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "0e99e71a-a1e7-402c-a85c-7c389921c987",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\udaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('tagsets')\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.data import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "fee61bb3-a4ec-4ae2-a839-f64885018886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LS', 'TO', 'VBN', \"''\", 'WP', 'UH', 'VBG', 'JJ', 'VBZ', '--', 'VBP', 'NN', 'DT', 'PRP', ':', 'WP$', 'NNPS', 'PRP$', 'WDT', '(', ')', '.', ',', '``', '$', 'RB', 'RBR', 'RBS', 'VBD', 'IN', 'FW', 'RP', 'JJR', 'JJS', 'PDT', 'MD', 'VB', 'WRB', 'NNP', 'EX', 'NNS', 'SYM', 'CC', 'CD', 'POS']\n"
     ]
    }
   ],
   "source": [
    "def get_tagsets():\n",
    "    tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
    "    return list(tagdict.keys())\n",
    "\n",
    "tag_list = get_tagsets()\n",
    "print(tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "c2559650-aad9-419d-a8ff-7181ea422358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_occu_freq(data, tag_list):\n",
    "    text_list = data.text\n",
    "    feature_df = pd.DataFrame(columns=tag_list)\n",
    "    for text_line in text_list:\n",
    "        pos_tags = [j for i,j in pos_tag(word_tokenize(text_line))]\n",
    "    \n",
    "        row = dict(Counter(pos_tags))\n",
    "        feature_df = feature_df.append(row, ignore_index = True)\n",
    "\n",
    "    feature_df.fillna(0, inplace=True)\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "521342fe-1a2e-4b0f-9034-335afd655305",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns=70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "65b2902d-bd90-4b93-8551-7d7407019a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Word-based tokenization is sufficient for many...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subword-based tokenization (e.g., BPE, WordPie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence tokenization is useful when sentence-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Character tokenization is useful in tasks wher...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Word-based tokenization is sufficient for many...\n",
       "1  Subword-based tokenization (e.g., BPE, WordPie...\n",
       "2  Sentence tokenization is useful when sentence-...\n",
       "3  Character tokenization is useful in tasks wher..."
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_list = get_tagsets()\n",
    "data = pd.read_csv('data.csv', header=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "9352b039-15c2-4585-a2f8-2e9e64610a3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18764\\306258049.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeature_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_pos_occu_freq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfeature_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18764\\914455369.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(data, tag_list)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtext_line\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mpos_tags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_line\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_tags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mfeature_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mfeature_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfeature_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6200\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6201\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6202\u001b[0m         ):\n\u001b[0;32m   6203\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6204\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "feature_df = get_pos_occu_freq(data, tag_list)\n",
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7733351-fed5-4d1c-a8dd-daa8959d7603",
   "metadata": {},
   "source": [
    "<h3> bag of words</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "835dd832-c4f4-45b0-b280-a9725c552db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "f955ef30-59ef-449a-b5fc-a5d553ed4841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(corpus):\n",
    "    bag_of_words_model = CountVectorizer()\n",
    "\n",
    "    dense_vec_matrix = bag_of_words_model.fit_transform(corpus).todense()\n",
    "    bag_of_words_df = pd.DataFrame(dense_vec_matrix)\n",
    "    bag_of_words_df.columns = sorted(bag_of_words_model.vocabulary_)\n",
    "    return bag_of_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "3976fa82-2698-4820-bc4c-43f3b3ae1f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "'''Word-based tokenization is sufficient for many applications like sentiment analysis, topic modeling, etc.''',\n",
    "'''Subword-based tokenization (e.g., BPE, WordPiece) is essential for transformer models (BERT, GPT) because it handles out-of-vocabulary words effectively.''',\n",
    "'''Sentence tokenization is useful when sentence-level understanding is needed, such as in text summarization.''',\n",
    "'''Character tokenization is useful in tasks where very fine granularity is needed, like handling noisy text or typos.'''\n",
    "]\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "6b4c8e79-4c97-43a6-9e02-524b4a26edf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analysis</th>\n",
       "      <th>applications</th>\n",
       "      <th>as</th>\n",
       "      <th>based</th>\n",
       "      <th>because</th>\n",
       "      <th>bert</th>\n",
       "      <th>bpe</th>\n",
       "      <th>character</th>\n",
       "      <th>effectively</th>\n",
       "      <th>essential</th>\n",
       "      <th>etc</th>\n",
       "      <th>fine</th>\n",
       "      <th>for</th>\n",
       "      <th>gpt</th>\n",
       "      <th>granularity</th>\n",
       "      <th>handles</th>\n",
       "      <th>handling</th>\n",
       "      <th>in</th>\n",
       "      <th>is</th>\n",
       "      <th>it</th>\n",
       "      <th>level</th>\n",
       "      <th>like</th>\n",
       "      <th>many</th>\n",
       "      <th>modeling</th>\n",
       "      <th>models</th>\n",
       "      <th>needed</th>\n",
       "      <th>noisy</th>\n",
       "      <th>of</th>\n",
       "      <th>or</th>\n",
       "      <th>out</th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>subword</th>\n",
       "      <th>such</th>\n",
       "      <th>sufficient</th>\n",
       "      <th>summarization</th>\n",
       "      <th>tasks</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenization</th>\n",
       "      <th>topic</th>\n",
       "      <th>transformer</th>\n",
       "      <th>typos</th>\n",
       "      <th>understanding</th>\n",
       "      <th>useful</th>\n",
       "      <th>very</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>when</th>\n",
       "      <th>where</th>\n",
       "      <th>word</th>\n",
       "      <th>wordpiece</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   analysis  applications  as  based  because  bert  bpe  character  \\\n",
       "0         1             1   0      1        0     0    0          0   \n",
       "1         0             0   0      1        1     1    1          0   \n",
       "2         0             0   1      0        0     0    0          0   \n",
       "3         0             0   0      0        0     0    0          1   \n",
       "\n",
       "   effectively  essential  etc  fine  for  gpt  granularity  handles  \\\n",
       "0            0          0    1     0    1    0            0        0   \n",
       "1            1          1    0     0    1    1            0        1   \n",
       "2            0          0    0     0    0    0            0        0   \n",
       "3            0          0    0     1    0    0            1        0   \n",
       "\n",
       "   handling  in  is  it  level  like  many  modeling  models  needed  noisy  \\\n",
       "0         0   0   1   0      0     1     1         1       0       0      0   \n",
       "1         0   0   1   1      0     0     0         0       1       0      0   \n",
       "2         0   1   2   0      1     0     0         0       0       1      0   \n",
       "3         1   1   2   0      0     1     0         0       0       1      1   \n",
       "\n",
       "   of  or  out  sentence  sentiment  subword  such  sufficient  summarization  \\\n",
       "0   0   0    0         0          1        0     0           1              0   \n",
       "1   1   0    1         0          0        1     0           0              0   \n",
       "2   0   0    0         2          0        0     1           0              1   \n",
       "3   0   1    0         0          0        0     0           0              0   \n",
       "\n",
       "   tasks  text  tokenization  topic  transformer  typos  understanding  \\\n",
       "0      0     0             1      1            0      0              0   \n",
       "1      0     0             1      0            1      0              0   \n",
       "2      0     1             1      0            0      0              1   \n",
       "3      1     1             1      0            0      1              0   \n",
       "\n",
       "   useful  very  vocabulary  when  where  word  wordpiece  words  \n",
       "0       0     0           0     0      0     1          0      0  \n",
       "1       0     0           1     0      0     0          1      1  \n",
       "2       1     0           0     1      0     0          0      0  \n",
       "3       1     1           0     0      1     0          0      0  "
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = vectorize_text(corpus)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "30f7c902-ab8c-44e2-9059-2f8a54c7c14f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>based</th>\n",
       "      <th>for</th>\n",
       "      <th>in</th>\n",
       "      <th>is</th>\n",
       "      <th>like</th>\n",
       "      <th>needed</th>\n",
       "      <th>sentence</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenization</th>\n",
       "      <th>useful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   based  for  in  is  like  needed  sentence  text  tokenization  useful\n",
       "0      1    1   0   1     1       0         0     0             1       0\n",
       "1      1    1   0   1     0       0         0     0             1       0\n",
       "2      0    0   1   2     0       1         2     1             1       1\n",
       "3      0    0   1   2     1       1         0     1             1       1"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bow_top_n(corpus, n):\n",
    "    bag_of_words_model_small = CountVectorizer(max_features=n)\n",
    "\n",
    "    dense_vec_matrix_small = bag_of_words_model_small.fit_transform(corpus).todense()\n",
    "    bag_of_words_df_small = pd.DataFrame(dense_vec_matrix_small)\n",
    "    bag_of_words_df_small.columns = sorted(bag_of_words_model_small.vocabulary_)\n",
    "    return bag_of_words_df_small\n",
    "\n",
    "df_2 = bow_top_n(corpus, 10)\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07b4504-b8b7-40b4-bfa5-58f940c4d404",
   "metadata": {},
   "source": [
    "<h3> TF - IDF(Term Frequency - Inverse Document Frequency) </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "222e4a9d-a7bc-4227-a50c-339e1e7d274e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "4a1419a2-67e9-47ea-8027-3cd057cdaec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_vectorizer(corpus):\n",
    "    tfidf_model = TfidfVectorizer()\n",
    "    vector_list = tfidf_model.fit_transform(corpus).todense()\n",
    "    return vector_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "de09021f-8b0a-4769-98c4-afc4105065ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.29605221 0.29605221 0.         0.23341096 0.         0.\n",
      "  0.         0.         0.         0.         0.29605221 0.\n",
      "  0.23341096 0.         0.         0.         0.         0.\n",
      "  0.15449233 0.         0.         0.23341096 0.29605221 0.29605221\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.29605221 0.         0.         0.29605221 0.\n",
      "  0.         0.         0.15449233 0.29605221 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.29605221 0.         0.        ]\n",
      " [0.         0.         0.         0.1869354  0.23710385 0.23710385\n",
      "  0.23710385 0.         0.23710385 0.23710385 0.         0.\n",
      "  0.1869354  0.23710385 0.         0.23710385 0.         0.\n",
      "  0.12373063 0.23710385 0.         0.         0.         0.\n",
      "  0.23710385 0.         0.         0.23710385 0.         0.23710385\n",
      "  0.         0.         0.23710385 0.         0.         0.\n",
      "  0.         0.         0.12373063 0.         0.23710385 0.\n",
      "  0.         0.         0.         0.23710385 0.         0.\n",
      "  0.         0.23710385 0.23710385]\n",
      " [0.         0.         0.26872437 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.21186538\n",
      "  0.28046304 0.         0.26872437 0.         0.         0.\n",
      "  0.         0.21186538 0.         0.         0.         0.\n",
      "  0.53744873 0.         0.         0.26872437 0.         0.26872437\n",
      "  0.         0.21186538 0.14023152 0.         0.         0.\n",
      "  0.26872437 0.21186538 0.         0.         0.26872437 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.26288899 0.         0.         0.         0.26288899\n",
      "  0.         0.         0.26288899 0.         0.26288899 0.2072647\n",
      "  0.27437276 0.         0.         0.2072647  0.         0.\n",
      "  0.         0.2072647  0.26288899 0.         0.26288899 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.26288899 0.2072647  0.13718638 0.         0.         0.26288899\n",
      "  0.         0.2072647  0.26288899 0.         0.         0.26288899\n",
      "  0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "vector_list = tf_idf_vectorizer(corpus)\n",
    "print(vector_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
